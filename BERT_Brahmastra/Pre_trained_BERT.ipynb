{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pre_trained_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cF3qC0Qd0gv_",
        "colab_type": "text"
      },
      "source": [
        "###Fine Tuning With BERT\n",
        "\n",
        "Lets Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdZuYPko0oV0",
        "colab_type": "text"
      },
      "source": [
        "##Theprocedure for the fine tune moves in the following wqays:-\n",
        "\n",
        "1. Getting the pretrained BERT model checkpoint\n",
        "\n",
        "2. Defining the specification of the tf.Module\n",
        "\n",
        "3. Exporting the module\n",
        "\n",
        "4. Building the text pre-processing pipeline\n",
        "\n",
        "5. Implementing a custom keras layer\n",
        "\n",
        "6. Training a Keras model to solve sentence pair classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPcG44gByyIm",
        "colab_type": "code",
        "outputId": "33070535-6ee8-433e-f525-fd3a231d5776",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'bert_repo'...\n",
            "remote: Enumerating objects: 336, done.\u001b[K\n",
            "Receiving objects:   0% (1/336)   \rReceiving objects:   1% (4/336)   \rReceiving objects:   2% (7/336)   \rReceiving objects:   3% (11/336)   \rReceiving objects:   4% (14/336)   \rReceiving objects:   5% (17/336)   \rReceiving objects:   6% (21/336)   \rReceiving objects:   7% (24/336)   \rReceiving objects:   8% (27/336)   \rReceiving objects:   9% (31/336)   \rReceiving objects:  10% (34/336)   \rReceiving objects:  11% (37/336)   \rReceiving objects:  12% (41/336)   \rReceiving objects:  13% (44/336)   \rReceiving objects:  14% (48/336)   \rReceiving objects:  15% (51/336)   \rReceiving objects:  16% (54/336)   \rReceiving objects:  17% (58/336)   \rReceiving objects:  18% (61/336)   \rReceiving objects:  19% (64/336)   \rReceiving objects:  20% (68/336)   \rReceiving objects:  21% (71/336)   \rReceiving objects:  22% (74/336)   \rReceiving objects:  23% (78/336)   \rReceiving objects:  24% (81/336)   \rReceiving objects:  25% (84/336)   \rReceiving objects:  26% (88/336)   \rReceiving objects:  27% (91/336)   \rReceiving objects:  28% (95/336)   \rReceiving objects:  29% (98/336)   \rReceiving objects:  30% (101/336)   \rReceiving objects:  31% (105/336)   \rReceiving objects:  32% (108/336)   \rReceiving objects:  33% (111/336)   \rReceiving objects:  34% (115/336)   \rReceiving objects:  35% (118/336)   \rReceiving objects:  36% (121/336)   \rReceiving objects:  37% (125/336)   \rReceiving objects:  38% (128/336)   \rReceiving objects:  39% (132/336)   \rReceiving objects:  40% (135/336)   \rReceiving objects:  41% (138/336)   \rReceiving objects:  42% (142/336)   \rReceiving objects:  43% (145/336)   \rReceiving objects:  44% (148/336)   \rReceiving objects:  45% (152/336)   \rReceiving objects:  46% (155/336)   \rReceiving objects:  47% (158/336)   \rReceiving objects:  48% (162/336)   \rReceiving objects:  49% (165/336)   \rReceiving objects:  50% (168/336)   \rReceiving objects:  51% (172/336)   \rReceiving objects:  52% (175/336)   \rReceiving objects:  53% (179/336)   \rReceiving objects:  54% (182/336)   \rReceiving objects:  55% (185/336)   \rReceiving objects:  56% (189/336)   \rremote: Total 336 (delta 0), reused 0 (delta 0), pack-reused 336\n",
            "Receiving objects:  57% (192/336)   \rReceiving objects:  58% (195/336)   \rReceiving objects:  59% (199/336)   \rReceiving objects:  60% (202/336)   \rReceiving objects:  61% (205/336)   \rReceiving objects:  62% (209/336)   \rReceiving objects:  63% (212/336)   \rReceiving objects:  64% (216/336)   \rReceiving objects:  65% (219/336)   \rReceiving objects:  66% (222/336)   \rReceiving objects:  67% (226/336)   \rReceiving objects:  68% (229/336)   \rReceiving objects:  69% (232/336)   \rReceiving objects:  70% (236/336)   \rReceiving objects:  71% (239/336)   \rReceiving objects:  72% (242/336)   \rReceiving objects:  73% (246/336)   \rReceiving objects:  74% (249/336)   \rReceiving objects:  75% (252/336)   \rReceiving objects:  76% (256/336)   \rReceiving objects:  77% (259/336)   \rReceiving objects:  78% (263/336)   \rReceiving objects:  79% (266/336)   \rReceiving objects:  80% (269/336)   \rReceiving objects:  81% (273/336)   \rReceiving objects:  82% (276/336)   \rReceiving objects:  83% (279/336)   \rReceiving objects:  84% (283/336)   \rReceiving objects:  85% (286/336)   \rReceiving objects:  86% (289/336)   \rReceiving objects:  87% (293/336)   \rReceiving objects:  88% (296/336)   \rReceiving objects:  89% (300/336)   \rReceiving objects:  90% (303/336)   \rReceiving objects:  91% (306/336)   \rReceiving objects:  92% (310/336)   \rReceiving objects:  93% (313/336)   \rReceiving objects:  94% (316/336)   \rReceiving objects:  95% (320/336)   \rReceiving objects:  96% (323/336)   \rReceiving objects:  97% (326/336)   \rReceiving objects:  98% (330/336)   \rReceiving objects:  99% (333/336)   \rReceiving objects: 100% (336/336)   \rReceiving objects: 100% (336/336), 290.61 KiB | 1.07 MiB/s, done.\n",
            "Resolving deltas:   0% (0/184)   \rResolving deltas:   2% (4/184)   \rResolving deltas:   4% (8/184)   \rResolving deltas:   5% (11/184)   \rResolving deltas:   7% (14/184)   \rResolving deltas:  13% (24/184)   \rResolving deltas:  15% (28/184)   \rResolving deltas:  17% (32/184)   \rResolving deltas:  23% (44/184)   \rResolving deltas:  24% (45/184)   \rResolving deltas:  26% (48/184)   \rResolving deltas:  31% (58/184)   \rResolving deltas:  35% (66/184)   \rResolving deltas:  45% (83/184)   \rResolving deltas:  48% (90/184)   \rResolving deltas:  57% (105/184)   \rResolving deltas:  99% (183/184)   \rResolving deltas: 100% (184/184)   \rResolving deltas: 100% (184/184), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcTzKkWy1UvF",
        "colab_type": "code",
        "outputId": "93aa6a4d-2f1f-46f4-c1ab-a244415690f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import re\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import auth, drive\n",
        "\n",
        "if not 'bert_repo' in sys.path:\n",
        "  sys.path.insert(0, 'bert_repo')\n",
        "\n",
        "from modeling import BertModel, BertConfig\n",
        "from tokenization import FullTokenizer, convert_to_unicode\n",
        "from extract_features import InputExample, convert_examples_to_features\n",
        "\n",
        "#Getting Tf Logger\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.handlers = []\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NmAgTey2vMW",
        "colab_type": "code",
        "outputId": "8b57ad16-9f84-4d40-d8ff-bb9ad4cab691",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "#Now lets move to our step 1 \n",
        "#Getting The pre-trained model\n",
        "\n",
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "!unzip uncased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-31 08:41:18--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.126.128, 2a00:1450:4013:c08::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.126.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M  81.1MB/s    in 4.8s    \n",
            "\n",
            "2020-01-31 08:41:24 (81.1 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
            "\n",
            "Archive:  uncased_L-12_H-768_A-12.zip\n",
            "   creating: uncased_L-12_H-768_A-12/\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYNxTRHL3ArT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now step 2 i.e. Building a TF Module\n",
        "'''These aredefined to provide a simpler way to manipulate usable part of\n",
        "the pre-trained model in Tensorflow. Goole maintains a curated lib of such modules in the tf.hub.\n",
        "'''\n",
        "\n",
        "def build_module_fn(config_path, vocab_path, do_lower_case=True):\n",
        "\n",
        "  def bert_module_fn(is_training):\n",
        "    \"\"\"This is the special function for token embeddings module.\"\"\"\n",
        "\n",
        "    input_ids = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"input_ids\")\n",
        "    input_mask = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"input_mask\")\n",
        "    token_type = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "    config = BertConfig.from_json_file(config_path)\n",
        "    model = BertModel(config=config, is_training=is_training,\n",
        "                      input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type)\n",
        "    \n",
        "    seq_output = model.all_encoder_layers[-1]\n",
        "    pool_output = model.get_pooled_output()\n",
        "\n",
        "    config_file = tf.constant(value=config_path, dtype=tf.string, name=\"config_file\")\n",
        "    vocab_file = tf.constant(value=vocab_path, dtype=tf.string, name=\"vocab_file\")\n",
        "    lower_case = tf.constant(do_lower_case)\n",
        "\n",
        "    tf.add_to_collection(tf.GraphKeys.ASSET_FILEPATHS, config_file)\n",
        "    tf.add_to_collection(tf.GraphKeys.ASSET_FILEPATHS, vocab_file)\n",
        "\n",
        "    input_map = {\"input_ids\": input_ids,\n",
        "                 \"input_mask\": input_mask,\n",
        "                 \"segment_ids\": token_type}\n",
        "        \n",
        "    output_map = {\"pooled_output\": pool_output,\n",
        "                  \"sequence_output\": seq_output}\n",
        "\n",
        "    output_info_map = {\"vocab_file\": vocab_file,\n",
        "                       \"do_lower_case\": lower_case}\n",
        "                \n",
        "    hub.add_signature(name=\"tokens\", inputs=input_map, outputs=output_map)\n",
        "    hub.add_signature(name=\"tokenization_info\", inputs={}, outputs=output_info_map)\n",
        "\n",
        "  return bert_module_fn    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0pkvr0uBVjQ",
        "colab_type": "code",
        "outputId": "fc2b1105-09ca-4981-f7f2-700a6e896561",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "#Now Lets move to step 3 for exporting the module\n",
        "\n",
        "MODEL_DIR = \"uncased_L-12_H-768_A-12\" #@param {type:\"string\"} ['uncased_L-12_H-768_A-12']\n",
        "\n",
        "config_path = \"/content/{}/bert_config.json\".format(MODEL_DIR)\n",
        "vocab_path = \"/content/{}/vocab.txt\".format(MODEL_DIR)\n",
        "\n",
        "tags_and_args = []\n",
        "for is_training in (True, False):\n",
        "  tags = set()\n",
        "  if is_training:\n",
        "    tags.add(\"train\")\n",
        "  tags_and_args.append((tags, dict(is_training=is_training)))\n",
        "\n",
        "module_fn = build_module_fn(config_path, vocab_path)\n",
        "spec = hub.create_module_spec(module_fn, tags_and_args=tags_and_args)\n",
        "spec.export(\"bert-module\", \n",
        "            checkpoint_path=\"/content/{}/bert_model.ckpt\".format(MODEL_DIR))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From bert_repo/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "From bert_repo/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "From bert_repo/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "From bert_repo/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "From bert_repo/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "From bert_repo/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "From /usr/local/lib/python3.6/dist-packages/tensorflow_hub/saved_model_lib.py:110: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ-YEiciCA9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Building The text preprocessing pipelines\n",
        "\n",
        "def read_examples(str_list):\n",
        "    \"\"\"Read a list of `InputExample`s from a list of strings.\"\"\"\n",
        "    unique_id = 0\n",
        "    for s in str_list:\n",
        "        line = convert_to_unicode(s)\n",
        "        if not line:\n",
        "            continue\n",
        "        line = line.strip()\n",
        "        text_a = None\n",
        "        text_b = None\n",
        "        m = re.match(r\"^(.*) \\|\\|\\| (.*)$\", line)\n",
        "        if m is None:\n",
        "            text_a = line\n",
        "        else:\n",
        "            text_a = m.group(1)\n",
        "            text_b = m.group(2)\n",
        "        yield InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b)\n",
        "        unique_id += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvLp8QALCLVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def features_to_arrays(features):\n",
        "\n",
        "    all_input_ids = []\n",
        "    all_input_mask = []\n",
        "    all_segment_ids = []\n",
        "\n",
        "    for feature in features:\n",
        "        all_input_ids.append(feature.input_ids)\n",
        "        all_input_mask.append(feature.input_mask)\n",
        "        all_segment_ids.append(feature.input_type_ids)\n",
        "\n",
        "    return (np.array(all_input_ids, dtype='int32'), \n",
        "            np.array(all_input_mask, dtype='int32'), \n",
        "            np.array(all_segment_ids, dtype='int32'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhjF2kSxCN5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now finally put all things togerther\n",
        "\n",
        "def build_preprocessor(voc_path, seq_len, lower=True):\n",
        "  tokenizer = FullTokenizer(vocab_file=voc_path, do_lower_case=lower)\n",
        "  \n",
        "  def strings_to_arrays(sents):\n",
        "  \n",
        "      sents = np.atleast_1d(sents).reshape((-1,))\n",
        "\n",
        "      examples = []\n",
        "      for example in read_examples(sents):\n",
        "          examples.append(example)\n",
        "\n",
        "      features = convert_examples_to_features(examples, seq_len, tokenizer)\n",
        "      arrays = features_to_arrays(features)\n",
        "      return arrays\n",
        "  \n",
        "  return strings_to_arrays"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHlf42flCVqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now Lets move to the step 5 for implementing the KERAS Bert file for the desired woerk\n",
        "\n",
        "class BertLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, bert_path, seq_len=64, n_tune_layers=3, \n",
        "                 pooling=\"cls\", do_preprocessing=True, verbose=False,\n",
        "                 tune_embeddings=False, trainable=True, **kwargs):\n",
        "\n",
        "        self.trainable = trainable\n",
        "        self.n_tune_layers = n_tune_layers\n",
        "        self.tune_embeddings = tune_embeddings\n",
        "        self.do_preprocessing = do_preprocessing\n",
        "\n",
        "        self.verbose = verbose\n",
        "        self.seq_len = seq_len\n",
        "        self.pooling = pooling\n",
        "        self.bert_path = bert_path\n",
        "\n",
        "        self.var_per_encoder = 16\n",
        "        if self.pooling not in [\"cls\", \"mean\", None]:\n",
        "            raise NameError(\n",
        "                f\"Undefined pooling type (must be either 'cls', 'mean', or None, but is {self.pooling}\"\n",
        "            )\n",
        "\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.bert = hub.Module(self.build_abspath(self.bert_path), \n",
        "                               trainable=self.trainable, name=f\"{self.name}_module\")\n",
        "\n",
        "        trainable_layers = []\n",
        "        if self.tune_embeddings:\n",
        "            trainable_layers.append(\"embeddings\")\n",
        "\n",
        "        if self.pooling == \"cls\":\n",
        "            trainable_layers.append(\"pooler\")\n",
        "\n",
        "        if self.n_tune_layers > 0:\n",
        "            encoder_var_names = [var.name for var in self.bert.variables if 'encoder' in var.name]\n",
        "            n_encoder_layers = int(len(encoder_var_names) / self.var_per_encoder)\n",
        "            for i in range(self.n_tune_layers):\n",
        "                trainable_layers.append(f\"encoder/layer_{str(n_encoder_layers - 1 - i)}/\")\n",
        "        \n",
        "        # Add module variables to layer's trainable weights\n",
        "        for var in self.bert.variables:\n",
        "            if any([l in var.name for l in trainable_layers]):\n",
        "                self._trainable_weights.append(var)\n",
        "            else:\n",
        "                self._non_trainable_weights.append(var)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"*** TRAINABLE VARS *** \")\n",
        "            for var in self._trainable_weights:\n",
        "                print(var)\n",
        "\n",
        "        self.build_preprocessor()\n",
        "        self.initialize_module()\n",
        "\n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def build_abspath(self, path):\n",
        "        if path.startswith(\"https://\") or path.startswith(\"gs://\"):\n",
        "          return path\n",
        "        else:\n",
        "          return os.path.abspath(path)\n",
        "\n",
        "    def build_preprocessor(self):\n",
        "        sess = tf.keras.backend.get_session()\n",
        "        tokenization_info = self.bert(signature=\"tokenization_info\", as_dict=True)\n",
        "        vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                              tokenization_info[\"do_lower_case\"]])\n",
        "        self.preprocessor = build_preprocessor(vocab_file, self.seq_len, do_lower_case)\n",
        "\n",
        "    def initialize_module(self):\n",
        "        sess = tf.keras.backend.get_session()\n",
        "        \n",
        "        vars_initialized = sess.run([tf.is_variable_initialized(var) \n",
        "                                     for var in self.bert.variables])\n",
        "\n",
        "        uninitialized = []\n",
        "        for var, is_initialized in zip(self.bert.variables, vars_initialized):\n",
        "            if not is_initialized:\n",
        "                uninitialized.append(var)\n",
        "\n",
        "        if len(uninitialized):\n",
        "            sess.run(tf.variables_initializer(uninitialized))\n",
        "\n",
        "    def call(self, input):\n",
        "\n",
        "        if self.do_preprocessing:\n",
        "          input = tf.numpy_function(self.preprocessor, \n",
        "                                    [input], [tf.int32, tf.int32, tf.int32], \n",
        "                                    name='preprocessor')\n",
        "          for feature in input:\n",
        "            feature.set_shape((None, self.seq_len))\n",
        "        \n",
        "        input_ids, input_mask, segment_ids = input\n",
        "        \n",
        "        bert_inputs = dict(\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
        "        )\n",
        "        output = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n",
        "        \n",
        "        if self.pooling == \"cls\":\n",
        "            pooled = output[\"pooled_output\"]\n",
        "        else:\n",
        "            result = output[\"sequence_output\"]\n",
        "            \n",
        "            input_mask = tf.cast(input_mask, tf.float32)\n",
        "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
        "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
        "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
        "            \n",
        "            if self.pooling == \"mean\":\n",
        "              pooled = masked_reduce_mean(result, input_mask)\n",
        "            else:\n",
        "              pooled = mul_mask(result, input_mask)\n",
        "\n",
        "        return pooled\n",
        "\n",
        "    def get_config(self):\n",
        "        config_dict = {\n",
        "            \"bert_path\": self.bert_path, \n",
        "            \"seq_len\": self.seq_len,\n",
        "            \"pooling\": self.pooling,\n",
        "            \"n_tune_layers\": self.n_tune_layers,\n",
        "            \"tune_embeddings\": self.tune_embeddings,\n",
        "            \"do_preprocessing\": self.do_preprocessing,\n",
        "            \"verbose\": self.verbose\n",
        "        }\n",
        "        super(BertLayer, self).get_config()\n",
        "        return config_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZFHuoXUCkiH",
        "colab_type": "code",
        "outputId": "d6567a0c-8a36-461e-cbb7-5e6137796bb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#This is the step 6for the sentence pair classification\n",
        "\n",
        "!wget http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv -O quora_train.tsv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-31 08:41:56--  http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv\n",
            "Resolving qim.fs.quoracdn.net (qim.fs.quoracdn.net)... 151.101.1.2, 151.101.65.2, 151.101.129.2, ...\n",
            "Connecting to qim.fs.quoracdn.net (qim.fs.quoracdn.net)|151.101.1.2|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58176133 (55M) [text/tab-separated-values]\n",
            "Saving to: ‘quora_train.tsv’\n",
            "\n",
            "quora_train.tsv     100%[===================>]  55.48M   165MB/s    in 0.3s    \n",
            "\n",
            "2020-01-31 08:41:58 (165 MB/s) - ‘quora_train.tsv’ saved [58176133/58176133]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuTPjOCBCt1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"quora_train.tsv\", sep='\\t')\n",
        "\n",
        "labels = df.is_duplicate.values\n",
        "\n",
        "texts = []\n",
        "delimiter = \" ||| \"\n",
        "for q1, q2 in zip(df.question1.tolist(), df.question2.tolist()):\n",
        "  texts.append(delimiter.join((str(q1), str(q2))))\n",
        "\n",
        "texts = np.array(texts)\n",
        "\n",
        "trX, tsX, trY, tsY = train_test_split(texts, labels, shuffle=True, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTUnkmGrCy1I",
        "colab_type": "code",
        "outputId": "45a0d93d-f9c9-437c-bd8b-50c1a35d8cb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "inp = tf.keras.Input(shape=(1,), dtype=tf.string)\n",
        "encoder = BertLayer(bert_path=\"./bert-module/\", seq_len=48, tune_embeddings=False,\n",
        "                    pooling='cls', n_tune_layers=3, verbose=False)\n",
        "\n",
        "pred = tf.keras.layers.Dense(1, activation='sigmoid')(encoder(inp))\n",
        "\n",
        "model = tf.keras.models.Model(inputs=[inp], outputs=[pred])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I5iqOGPC2LY",
        "colab_type": "code",
        "outputId": "730a2308-ddc0-469a-84f5-9981bf422ffa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5, ),\n",
        "      loss=\"binary_crossentropy\",\n",
        "      metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1)]               0         \n",
            "_________________________________________________________________\n",
            "bert_layer (BertLayer)       (None, 768)               109482240 \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 769       \n",
            "=================================================================\n",
            "Total params: 109,483,009\n",
            "Trainable params: 21,854,977\n",
            "Non-trainable params: 87,628,032\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJ9lb21zC5kg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fdxRpFMC90_",
        "colab_type": "code",
        "outputId": "f54d68f4-c6b9-4a7d-82cd-e3851dc3ea6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "saver = keras.callbacks.ModelCheckpoint(\"bert_tuned.hdf5\")\n",
        "\n",
        "model.fit(trX, trY, validation_data=[tsX, tsY], batch_size=128, epochs=5, callbacks=[saver])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 323432 samples, validate on 80858 samples\n",
            "Epoch 1/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From bert_repo/extract_features.py:283: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "323432/323432 [==============================] - 1102s 3ms/sample - loss: 0.3643 - acc: 0.8256 - val_loss: 0.3213 - val_acc: 0.8542\n",
            "Epoch 2/5\n",
            "323432/323432 [==============================] - 1100s 3ms/sample - loss: 0.2872 - acc: 0.8722 - val_loss: 0.2954 - val_acc: 0.8686\n",
            "Epoch 3/5\n",
            "323432/323432 [==============================] - 1096s 3ms/sample - loss: 0.2461 - acc: 0.8936 - val_loss: 0.2814 - val_acc: 0.8776\n",
            "Epoch 4/5\n",
            "323432/323432 [==============================] - 1095s 3ms/sample - loss: 0.2073 - acc: 0.9133 - val_loss: 0.2861 - val_acc: 0.8788\n",
            "Epoch 5/5\n",
            "323432/323432 [==============================] - 1095s 3ms/sample - loss: 0.1671 - acc: 0.9332 - val_loss: 0.3051 - val_acc: 0.8803\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdca831c9b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIb0lszdDAqw",
        "colab_type": "code",
        "outputId": "7270d244-e606-4ebc-cdc1-657ecaa97369",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "source": [
        "#This is step 7 of the pre trained module part which is basically the saving and the restoring part\n",
        "\n",
        "model.predict(trX[:10])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-07faf69a0ea2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJxabxeLDNlg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "json.dump(model.to_json(), open(\"model.json\", \"w\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI7N-2iiDSAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.model_from_json(json.load(open(\"model.json\")), \n",
        "                                        custom_objects={\"BertLayer\": BertLayer})\n",
        "\n",
        "model.load_weights(\"bert_tuned.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUAXOj-WDUXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.predict(trX[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfI09tocDXfA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"During \"freezing\" the model variables are replaced by constants, and the nodes required for training are pruned from the computational graph. The resulting graph becomes more lightweight, requires less RAM and achieves better performance.\"\"\"\n",
        "\n",
        "from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
        "from tensorflow.python.tools.optimize_for_inference_lib import optimize_for_inference\n",
        "\n",
        "def freeze_keras_model(model, export_path=None, clear_devices=True):\n",
        "    \"\"\"\n",
        "    Freezes a Keras model into a pruned computation graph.\n",
        "\n",
        "    @param model The Keras model to be freezed.\n",
        "    @param clear_devices Remove the device directives from the graph for better portability.\n",
        "    @return The frozen graph definition.\n",
        "    \"\"\"\n",
        "    \n",
        "    sess = tf.keras.backend.get_session()\n",
        "    graph = sess.graph\n",
        "    \n",
        "    with graph.as_default():\n",
        "\n",
        "        input_tensors = model.inputs\n",
        "        output_tensors = model.outputs\n",
        "        dtypes = [t.dtype.as_datatype_enum for t in input_tensors]\n",
        "        input_ops = [t.name.rsplit(\":\", maxsplit=1)[0] for t in input_tensors]\n",
        "        output_ops = [t.name.rsplit(\":\", maxsplit=1)[0] for t in output_tensors]\n",
        "        \n",
        "        tmp_g = graph.as_graph_def()\n",
        "        if clear_devices:\n",
        "            for node in tmp_g.node:\n",
        "                node.device = \"\"\n",
        "        \n",
        "        tmp_g = optimize_for_inference(\n",
        "            tmp_g, input_ops, output_ops, dtypes, False)\n",
        "        \n",
        "        tmp_g = convert_variables_to_constants(sess, tmp_g, output_ops)\n",
        "        \n",
        "        if export_path is not None:\n",
        "            with tf.gfile.GFile(export_path, \"wb\") as f:\n",
        "                f.write(tmp_g.SerializeToString())\n",
        "        \n",
        "        return tmp_g"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgQaLwNtDi_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frozen_graph = freeze_keras_model(model, export_path=\"frozen_graph.pb\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZwFsQ5vDl8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/gaphex/bert_experimental/\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "sys.path.insert(0, \"/content/bert_experimental\")\n",
        "\n",
        "from bert_experimental.finetuning.text_preprocessing import build_preprocessor\n",
        "from bert_experimental.finetuning.graph_ops import load_graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nekbq5gJDo-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "restored_graph = load_graph(\"frozen_graph.pb\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En-9pcAqDqyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph_ops = restored_graph.get_operations()\n",
        "input_op, output_op = graph_ops[0].name, graph_ops[-1].name\n",
        "print(input_op, output_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmfv0W-TDs0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = restored_graph.get_tensor_by_name(input_op + ':0')\n",
        "y = restored_graph.get_tensor_by_name(output_op + ':0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0HK0Wx3DveX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessor = build_preprocessor(\"./uncased_L-12_H-768_A-12/vocab.txt\", 64)\n",
        "py_func = tf.numpy_function(preprocessor, [x], [tf.int32, tf.int32, tf.int32], name='preprocessor')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZxVe48fDxZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "py_func = tf.numpy_function(preprocessor, [x], [tf.int32, tf.int32, tf.int32])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXzAi3c9DzeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#finally get the predictions\n",
        "\n",
        "sess = tf.Session(graph=restored_graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJi42YufD38C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trX[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkh6xh6QD5vQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_out = sess.run(y, feed_dict={\n",
        "        x: trX[:10].reshape((-1,1))\n",
        "    })\n",
        "\n",
        "y_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZeciBHJD7U_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}